{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4.2.5 Clustering Challenge: Boston Marathon\n",
    "\n",
    "You now have a pretty varied suite of clustering and clustering evaluation methods; we'd be remiss if we didn't give you the opportunity to try them out on some real data. So here we go!\n",
    "\n",
    "There is a lot of information on [runners and their performance for the Boston Marathon](https://github.com/llimllib/bostonmarathon). Pick a year (post-2012 has more info) and do some clustering.\n",
    "\n",
    "Specifically, use the tools at hand to determine which clustering solution, including number of clusters and algorithm used, is best for the marathon data. Once you have a solution you like, write a data story, including visualizations, where you teach the reader something about the Boston Marathon based on your clusters. Write up your report, including your process from start to finish, in a Jupyter notebook and submit it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "  function code_toggle() {\n",
       "    if (code_shown){\n",
       "      $('div.input').hide('500');\n",
       "      $('#toggleButton').val('Show Code')\n",
       "    } else {\n",
       "      $('div.input').show('500');\n",
       "      $('#toggleButton').val('Hide Code')\n",
       "    }\n",
       "    code_shown = !code_shown\n",
       "  }\n",
       "\n",
       "  $( document ).ready(function(){\n",
       "    code_shown=false;\n",
       "    $('div.input').hide()\n",
       "  });\n",
       "</script>\n",
       "<form action=\"javascript:code_toggle()\"><input type=\"submit\" id=\"toggleButton\"\n",
       "value=\"Show Code\"></form>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<script>\n",
    "  function code_toggle() {\n",
    "    if (code_shown){\n",
    "      $('div.input').hide('500');\n",
    "      $('#toggleButton').val('Show Code')\n",
    "    } else {\n",
    "      $('div.input').show('500');\n",
    "      $('#toggleButton').val('Hide Code')\n",
    "    }\n",
    "    code_shown = !code_shown\n",
    "  }\n",
    "\n",
    "  $( document ).ready(function(){\n",
    "    code_shown=false;\n",
    "    $('div.input').hide()\n",
    "  });\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" id=\"toggleButton\"\n",
    "value=\"Show Code\"></form>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mpcol\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Data Overview\n",
    "\n",
    "Let's take a look at a preview of our data to see what we are dealing with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"https://raw.githubusercontent.com/llimllib/bostonmarathon/master/results/2014/results.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Data Cleaning\n",
    "\n",
    "Let's clean up the data by converting some data types and restructuring some columns. This will help us run our models and create some nice visualizations.<br><br>\n",
    "\n",
    "First let's start by creating a heatmap to see which columns are missing significant data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot if any columns have missing data\n",
    "plt.figure(figsize=(15,4))\n",
    "sns.heatmap(df.isnull(), cbar = False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this, it seems we can delete the CTZ column as nearly every single row is missing data in that column. As for the STATE column, it makes sense that runners from countries outside the US would have empty values in the STATE column so we will let that slide here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the CTZ column as most of the values in that column are NaN.\n",
    "df.drop(['ctz'], 1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's continue by making sure all checkpoint data is in a numerical type. The checkpoint data is currently split into eight seperate columns and spread out all over the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a variable for all checkpoint data and convert them to FLOAT dtypes. \n",
    "# The 'OFFICIAL' column is already an INT so we will add it in later.\n",
    "checkpoints = ['5k', '10k', '20k', 'half', '25k', '30k', '35k', '40k']\n",
    "for i in checkpoints:\n",
    "    df[i] = df[i].str.replace('-','0.0').astype(float)\n",
    "    df[i] = df[i].astype(float)\n",
    "\n",
    "# Add the 'OFFICIAL' column into the CHECKPOINTS variable\n",
    "checkpoints.append('official')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[checkpoints].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's convert the values in the AGE column into age brackets. This will allow us to minimize the unique values in the column and will provide better visualization capabilities. To do so, let's use the age brackets created by the Boston Athletic Association as seen [here](https://www.baa.org/races/boston-marathon/enter/qualify/history-qualifying-times)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of the age brackets and the ages assigned to them\n",
    "age_brackets = {'18-34':(18,34),\n",
    "                '35-39':(35,39),\n",
    "                '40-44':(40,44),\n",
    "                '45-49':(45,49),\n",
    "                '50-54':(50,54),\n",
    "                '55-59':(55,59),\n",
    "                '60-64':(60,64),\n",
    "                '65-69':(65,69),\n",
    "                '70-74':(70,74),\n",
    "                '75-79':(75,79),\n",
    "                '80+':(80,84)}\n",
    "\n",
    "# Create a function that will assign each age value into an age bracket\n",
    "def age_to_bracket(x):    \n",
    "    for bracket in age_brackets: \n",
    "        if age_brackets[bracket][0] <= x <= age_brackets[bracket][1]:\n",
    "            return bracket               \n",
    "    else: \n",
    "        print('Age value -{}- could not be assigned into a bucket'.format(x))\n",
    "        return np.nan\n",
    "\n",
    "# Create a new column using the values created by applying the age bracket function\n",
    "df['Age Bracket'] = df['age'].apply(lambda x: age_to_bracket(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Age Bracket'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's convert the values in the BIB column into wave brackets. This will also allow us to minimize the unique values in the column and will provide better visualization capabilities. To do so, let's use the wave brackets created by the Boston Athletic Association as seen [here](http://registration.baa.org/2014/cf/Public/iframe_EntryLists.cfm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wave_brackets = {'Elite':(0,100), \n",
    "                 'Wave1':(101,8999), \n",
    "                 'Wave2':(9000,17999), \n",
    "                 'Wave3':(18000,26999), \n",
    "                 'Wave4':(27000,50000)}\n",
    "\n",
    "# Create a function that will assign each bib value into a wave bracket\n",
    "def bib_to_wave(x):   \n",
    "    try:\n",
    "        for wave in wave_brackets:\n",
    "            if wave_brackets[wave][0] <= float(x) <= wave_brackets[wave][1]: return wave\n",
    "    except ValueError:\n",
    "        if 'W' or 'F' in str(x): return 'Elite'\n",
    "    else: \n",
    "        print(float(x))\n",
    "        return np.nan\n",
    "\n",
    "# Create a new column using the values created by applying the wave bracket function\n",
    "df['Wave'] = df['bib'].apply(lambda x: bib_to_wave(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Wave'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will convert the values in the COUNTRY column. According to the Boston Athletic Association, only five countries had over 200 participants in the 2014 Boston Marathon - __USA, Canada, United Kingdom, Italy, and Mexico__. Therefore, let's convert all other COUNTRY values to now say \"OTHER\". This will also allow us to minimize the unique values in the column and will provide better visualization capabilities.<br><br>\n",
    "\n",
    "After that we will convert the __Gender__ column into binary 1 / 0 for use in our clustering models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a variable of the country tags we will be using\n",
    "countries = ['USA', 'CAN', 'GBR', 'ITA', 'MEX']\n",
    "\n",
    "# Assign all other countries with the value \"OTHER\"\n",
    "df['Country'] = df['country'].apply(lambda x: np.where(x not in countries, 'OTHER', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Country'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarize the gender variable for use in our cluster modeling\n",
    "df['female'] = df['gender'].apply(lambda x: np.where(x == 'F', 1, 0))\n",
    "df['female'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Data Analysis\n",
    "\n",
    "It looks like our data is now relatively clean. Let's make some visualization to have a better understanding of the dataset we are dealing with.<br><br>\n",
    "\n",
    "First let's see how the participant counts compare by __gender__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a countplot for Male and Female participants\n",
    "plt.figure(figsize=(15,4))\n",
    "sns.countplot(y=df['gender'], palette = sns.cubehelix_palette(2, rot = -0.5))\n",
    "plt.title('Participant Count by Gender', fontsize=16)\n",
    "plt.xlabel('Number of Participants')\n",
    "plt.ylabel('Gender')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's see the __age count by gender__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a countplot for participant ages by gender\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "fig, (ax0, ax1) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "fig.suptitle('Participant Count by Age Bracket and Gender', fontsize=16)\n",
    "\n",
    "male   = df[df['gender'] == 'M']['Age Bracket']\n",
    "female = df[df['gender'] == 'F']['Age Bracket']\n",
    "frames = [male, female]\n",
    "labels = ['Male', 'Female']\n",
    "\n",
    "palettes = {}\n",
    "palettes['Male']   = sns.cubehelix_palette(len(age_brackets), start=1, rot=-0.75, \n",
    "                                           light=0.8, dark=0.2)\n",
    "palettes['Female'] = sns.cubehelix_palette(len(age_brackets))\n",
    "\n",
    "for idx, ax in enumerate([ax0, ax1]):\n",
    "    sns.countplot(frames[idx], ax = ax, \n",
    "                  order = age_brackets,\n",
    "                  palette = palettes[labels[idx]],\n",
    "                  label = labels[idx])\n",
    "    ax.set(ylim = (0, 5000), ylabel = 'Number of Participants', xlabel = 'Age')\n",
    "    ax.set_title(labels[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here let's make a barplot of the participants' nationalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a value-count table to see the counts of runners from each country\n",
    "country_sort = df['Country'].value_counts(ascending=True)\n",
    "country_sort = country_sort.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a countplot for Male and Female participants\n",
    "plt.figure(figsize=(15,5))\n",
    "sns.barplot(x = 'Country', y = country_sort.index, data = country_sort)\n",
    "plt.title('Participant Count by Country', fontsize=16)\n",
    "plt.xlabel('Number of Participants')\n",
    "plt.ylabel('Country')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how many runners participated in each wave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe of the wave information\n",
    "wave_df = df['Wave'].value_counts().to_frame()\n",
    "\n",
    "# Create a pie chart for waves\n",
    "plt.pie(wave_df['Wave'], explode = (0.15, 0.15, 0.15, 0.15, 0.15), labels = wave_df.index,\n",
    "autopct = '%1.1f%%', shadow = True, startangle = 45)\n",
    "plt.title('Participants by Wave', fontsize=16) \n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here let's see how each gender faired at each checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe that melts the checkpoint data by gender\n",
    "stacked_checkpoints = pd.melt(df, value_vars = checkpoints, var_name='checkpoint',\n",
    "                              value_name='time', id_vars=['gender'])\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "sns.pointplot(data=stacked_checkpoints, x='checkpoint', y='time', hue='gender',\n",
    "                   markers=['.','.'], linestyles=['--', ':'], ci=95, orient=\"v\", \n",
    "                   dodge=0.25)\n",
    "\n",
    "plt.title('Checkpoint Times by Gender', fontsize=16)\n",
    "plt.ylabel('Time in Minutes')\n",
    "plt.xlabel('Checkpoint')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import pairwise_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's create a dataframe of all the features we will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a blank dataframe called 'modeling_df'\n",
    "modeling_df = pd.DataFrame()\n",
    "\n",
    "# Add the columns that are already in the correct format\n",
    "modeling_df['age'] = df['age']\n",
    "modeling_df['female'] = df['female']\n",
    "modeling_df['division'] = df['division']\n",
    "modeling_df['genderdiv'] = df['genderdiv']\n",
    "modeling_df['overall'] = df['overall']\n",
    "\n",
    "# Add checkpoint values as seperate columns\n",
    "for checkpoint in checkpoints:\n",
    "    modeling_df[checkpoint] = df[checkpoint]\n",
    "\n",
    "# Add waves as seperate columns\n",
    "for wave in df['Wave'].unique():\n",
    "    modeling_df[wave] = np.where(df['Wave'] == wave, 1, 0)\n",
    "\n",
    "# Add countries with significant levels of participation as binary columns\n",
    "# First add the 'OTHER' country value into the variable called 'countries'\n",
    "countries.append('OTHER')\n",
    "for country in countries:\n",
    "    modeling_df[country] = np.where(df['Country'] == country, 1, 0)\n",
    "\n",
    "modeling_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of our features span different scales so let's normalize the data so we can compare the features against each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "X = modeling_df\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "np_scaled = min_max_scaler.fit_transform(X)\n",
    "X_normed = pd.DataFrame(np_scaled, columns=list(modeling_df))\n",
    "X_normed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conduct a Principal Component Analysis so we can plot the data on a two dimensional graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coduct PCA on the normalized dataframe\n",
    "pca = PCA(n_components = 2)\n",
    "X_pca = pca.fit_transform(X_normed)\n",
    "pca_1 = [x[0] for x in X_pca]\n",
    "pca_2 = [x[1] for x in X_pca]\n",
    "X_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Clustering\n",
    "## Mean Shift Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Shift Clustering\n",
    "bandwidth = estimate_bandwidth(X_normed, quantile = 0.21, n_samples = 500, n_jobs = 1)\n",
    "\n",
    "# Fit the model\n",
    "ms = MeanShift(bandwidth = bandwidth, bin_seeding = True)\n",
    "ms.fit(X_normed)\n",
    "\n",
    "# Extract cluster assignments for each datapoint\n",
    "labels = ms.labels_\n",
    "\n",
    "# Coordinates of the cluser centers\n",
    "cluster_centers = ms.cluster_centers_\n",
    "\n",
    "# Count our clusters\n",
    "n_clusters_ = len(np.unique(labels))\n",
    "print('Number of estimated clusters: {}'.format(n_clusters_))\n",
    "\n",
    "ms_labels = ms.predict(X_normed)\n",
    "\n",
    "ms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the clusters look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the clusters to the dataframes\n",
    "modeling_df['cluster'] = ms_labels + 1\n",
    "df['cluster'] = ms_labels + 1\n",
    "\n",
    "# Generate color palettes for the plots\n",
    "palettes['clust_rgb'] = sns.color_palette('Set2', n_clusters_).as_hex()\n",
    "palettes['clust_cmap'] = mpcol.ListedColormap(palettes['clust_rgb'], name='my_name')\n",
    "\n",
    "plt.figure(figsize=(15,6))\n",
    "\n",
    "# Create a countplot of the clusters\n",
    "ax1 = plt.subplot(1, 2, 1)    \n",
    "sns.countplot(x = modeling_df['cluster'], palette=sns.color_palette(palettes['clust_rgb']))\n",
    "\n",
    "plt.title('Participants per Cluster')\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Number of Participants')\n",
    "\n",
    "# Visualize the clusters\n",
    "ax2 = plt.subplot(1, 2, 2)    \n",
    "plt.scatter(x = pca_1, y = pca_2, c= modeling_df['cluster'], \n",
    "            cmap = palettes['clust_cmap'], alpha=0.1)\n",
    "plt.title('Cluster Shapes')\n",
    "plt.xlabel('PCA Dimension 1')\n",
    "plt.ylabel('PCA Dimension 2')\n",
    "ax2.yaxis.tick_right()\n",
    "ax2.yaxis.set_label_position(\"right\")\n",
    "plt.suptitle('Mean Shift Cluster Analysis', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Being that we cannot verify the cluster accuracy against any originally-dictated values, we are forced to use a less accurate measuring technique. Here we will use the Silhouette Coefficient method. This method rates each point's distance from points in neighboring clusters on a scale of -1 to +1. We will take four equally-sized samples and find the average of the Silhouette Coefficients for all point in the sample. If the values are consistent, we can determine that the clusters are accurate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data in half\n",
    "X_half1, X_half2, X_pcahalf1, X_pcahalf2 = train_test_split(\n",
    "    X_normed,\n",
    "    X_pca,\n",
    "    test_size=0.5,\n",
    "    random_state=42)\n",
    "\n",
    "# We want to create four samples so lets split the two we just created in half as well\n",
    "X1, X2, X_pca1, X_pca2 = train_test_split(\n",
    "    X_half1,\n",
    "    X_pcahalf1,\n",
    "    test_size=0.5,\n",
    "    random_state=42)\n",
    "X3, X4, X_pca3, X_pca4 = train_test_split(\n",
    "    X_half2,\n",
    "    X_pcahalf2,\n",
    "    test_size=0.5,\n",
    "    random_state=42)\n",
    "\n",
    "counter = 0\n",
    "print('SILHOUETTE COEFFICIENTS')\n",
    "print('-----------------------')\n",
    "for sample in [X1, X2, X3, X4]:\n",
    "    model = MeanShift(bandwidth=bandwidth, bin_seeding=True).fit(sample)\n",
    "    labels = model.labels_\n",
    "    counter += 1\n",
    "    print('Sample {}: {}'.format(counter, round(metrics.silhouette_score(sample, labels, metric='euclidean'),4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the coefficients are positive values and somewhat consistent which indicates that the clustering has a possibility of being accurate!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6) Outcome Summary\n",
    "## 6A - Cluster Demographics\n",
    "Let's take a look at how the clusters compare demographically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a figure summarizing demographics by cluster\n",
    "fig = plt.figure(figsize=(15,12))\n",
    "gs = GridSpec(1, 4)\n",
    "cluster_labels = ['{}'.format(x) for x in range(1, n_clusters_ + 1)]\n",
    "gender_labels = ['', '1', '', '2', '', '3', '', '4']\n",
    "\n",
    "# Country\n",
    "ctry_data = {}\n",
    "for cluster in np.arange(1, n_clusters_ + 1):\n",
    "    ctry_data[cluster] = [(df[(df['cluster'] == cluster) & (df['Country'] == country)]['Country'].count())\n",
    "                          /df[df['cluster'] == cluster].shape[0] for country in countries]\n",
    "# Plot the data\n",
    "ax1 = plt.subplot(2, 2, 1)\n",
    "sns.heatmap(pd.DataFrame(ctry_data, index = countries).transpose(), annot=True, fmt=\".0%\",\n",
    "            cmap = sns.light_palette(\"green\"), cbar = False, vmin = 0, vmax = 0.15)\n",
    "ax1.set_yticklabels(cluster_labels)\n",
    "plt.yticks(rotation=0) \n",
    "plt.title('Country')\n",
    "\n",
    "# Age\n",
    "# Create a placeholder column\n",
    "modeling_df['y'] = 1\n",
    "ax2 = plt.subplot(2, 2, 2)    \n",
    "sns.boxplot(y = 'y', x = 'age', data = modeling_df, hue = 'cluster',\n",
    "            palette = sns.color_palette(palettes['clust_rgb']), orient = 'h',\n",
    "            saturation = 1)\n",
    "ax2.axes.get_yaxis().set_visible(False)\n",
    "plt.title('Age')\n",
    "plt.xlabel('Age in Years')\n",
    "plt.legend(loc = \"upper right\", title = 'Cluster')\n",
    "\n",
    "# Gender\n",
    "ax3 = plt.subplot(2, 1, 2)\n",
    "for cluster in np.arange(n_clusters_ + 1):\n",
    "    pct_female = modeling_df[modeling_df['cluster'] == cluster]['female'].mean()\n",
    "    std_female = modeling_df[modeling_df['cluster'] == cluster]['female'].std()\n",
    "    pct_male   = 1 - pct_female\n",
    "    std_male   = 1 - std_female\n",
    "    \n",
    "    pfemale = plt.barh(cluster, pct_female, 0.5, color = '#e57068')\n",
    "    pmale   = plt.barh(cluster, pct_male, 0.5, left = pct_female, color = '#c5c2f4')\n",
    "\n",
    "plt.gca().invert_yaxis()\n",
    "ax3.set_yticklabels(gender_labels)\n",
    "ax3.set_xticklabels(['{}%'.format(tick) for tick in np.arange(0,120,20)])\n",
    "gender_legend = [mpatches.Patch(color = '#c5c2f4', label = 'Male'),\n",
    "                 mpatches.Patch(color = '#e57068', label = 'Female')]\n",
    "plt.legend(handles = gender_legend, loc = \"lower right\", title='Gender')\n",
    "plt.title('Gender')\n",
    "\n",
    "# Define overall aesthetics and plot.\n",
    "plt.suptitle('Cluster Demographics', fontsize=22, y=1.05)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6B - Cluster Performance\n",
    "Let's take a look at how the clusters' performance in the marathon compare with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a marathon performance cluster comparison graph\n",
    "gs = GridSpec(1, 3)\n",
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "# Stack the checkpoint data into a single dataframe\n",
    "stacked_checkpoints = pd.melt(df, value_vars = checkpoints, var_name = 'checkpoint',\n",
    "                              value_name = 'time', id_vars = ['cluster'])\n",
    "\n",
    "## CHECKPOINT PLOT\n",
    "# Plot the average checkpoint times for each wave using the new stacked data\n",
    "ax1 = plt.subplot(gs[0, 0:2])    \n",
    "sns.pointplot(data = stacked_checkpoints, x = 'checkpoint', y = 'time', hue = 'cluster',\n",
    "              palette = sns.color_palette(palettes['clust_rgb']), ci = 95, orient = \"v\",\n",
    "              dodge = 0.5)\n",
    "plt.title('Average Checkpoint Completion Time')\n",
    "plt.ylabel('Time in Minutes')\n",
    "plt.xlabel('Checkpoint')\n",
    "\n",
    "lg = ax1.legend(title = 'Clusters', markerscale = 2, fontsize = 12)\n",
    "title = lg.get_title()\n",
    "title.set_fontsize(12)\n",
    "\n",
    "## PACE PLOT\n",
    "# Create a placeholder column\n",
    "df['y'] = modeling_df['y']\n",
    "ax2 = plt.subplot(gs[0, 2])\n",
    "sns.pointplot(x = 'y', y = 'pace', hue = 'cluster', data = df, ci = 'sd',\n",
    "              palette = sns.color_palette(palettes['clust_rgb']), dodge = 0.5)\n",
    "ax2.legend_.remove()\n",
    "ax2.set_xticklabels('')\n",
    "ax2.axes.get_xaxis().set_visible(False)\n",
    "\n",
    "plt.ylabel('Average Pace (MPH)')\n",
    "plt.title('Pace')\n",
    "\n",
    "# OVERALL PLOTS INFORMATION\n",
    "plt.suptitle('Marathon Performance by Cluster',fontsize=22, y=1.05)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6C - Cluster Profiles\n",
    "Let's take a deeper look into the average feature values for each cluster as well as how many participants in each cluster were in each wave and from which country. Let's see what we learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pivot table based on cluster averages\n",
    "cluster_df = df.groupby(['cluster']).mean()\n",
    "cluster_df = cluster_df[['age', 'female', 'overall', 'division', 'genderdiv', 'half',\n",
    "                         'official', 'pace']]\n",
    "cluster_df['mph'] = round(60 / cluster_df['pace'], 2)\n",
    "cluster_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pivot table based on participants in each wave and cluster\n",
    "cluster_wave = df.pivot_table(values = 'name', columns = 'Wave', index = 'cluster',\n",
    "                              aggfunc = 'count', margins = True)\n",
    "cluster_wave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pivot table based on participants in each wave and cluster\n",
    "cluster_cntry = df.pivot_table(values = 'name', columns = 'Country', index = 'cluster',\n",
    "                              aggfunc = 'count', margins = True)\n",
    "cluster_cntry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🍺 Cluster 1 - _\"I'm here for fun\"_\n",
    "Cluster 1 includes __85% of all participants__. This cluster has an average age of __42 years old__, with a fairly even split down the middle between males and females. These participants were not in any rush to complete the marathon and none of them were part of the Elite wave. The average pace for this cluster was 9 minutes and 22 seconds per mile, which reflects an average speed of __6.4 miles per hour__. Knowing this, it makes sense that over 99.7% of participants in this cluster are from the USA. It is very unlikely for someone to fly in from a foreign country to participate in a marathon simply to have some fun and not take it seriously.\n",
    "\n",
    "### 👍 Cluster 2 - _\"I'm here for fun but I'll make an effort\"_\n",
    "Cluster 2 makes up __7.5% of all marathon participants__. This cluster has an average age of __47 years old__, with a __55/45 split males to females__. These participants were a bit quicker than those in Cluster 1 and does not include any participants from the Elite wave. The average pace for this cluster was 8 minutes and 46 seconds per mile, which reflects an average speed of __6.9 miles per hour__. This group is made up of those who have a similar outlook to those in Cluster 1 but they have come from other countries. This cluster __does not include anyone from the USA__. This makes sense being that if these participants are already traveling from abroad, they have some significant interest in actually participating in the marathon, and therefore, have better average paces than those in Cluster 1.\n",
    "\n",
    "### 🧓 Cluster 3 - _\"I was a runner when I was young\"_\n",
    "Cluster 3 makes up about __7% of all attendees__. This cluster has an average age of __46 years old__, with an __81/19 split between males to females__. This cluster was about the same speed as Cluster 1 and does not include any participants from the Elite wave. The average pace for this cluster was 8 minutes and 56 seconds per mile, which reflects an average speed of __6.7 miles per hour__. Interestingly enough, when comparing the average half times from the average finish times, this cluster had the largest slow-down in the second half of the marathon. Based on these findings, it seems that these participants might possibly have been marathon runners in previous years.\n",
    "\n",
    "### 🏃 Cluster 4 - _\"I trained for this\"_\n",
    "Cluster 4 makes up only __0.5% of the entire marathon__. This cluster has an average age of __34 years old__, with a __60/40 split between males and females__. These participants definitely trained for this event as they are __all part of the Elite wave__. The average pace for this cluster was 5 minutes and 10 seconds per mile, which reflects an average speed of __11.6 miles per hour__! Being that these runners are serious about marathons, it is understandable that this is where many of the foreigners can be found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
